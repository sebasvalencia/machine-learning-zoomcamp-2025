{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58cb20d",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "In this dataset our desired target for classification task will be converted variable - has the client signed up to the platform or not.\n",
    "\n",
    "Data preparation:\n",
    "\n",
    "- Check if the missing values are presented in the features.\n",
    "- If there are missing values:\n",
    "    - For caterogiral features, replace them with 'NA'\n",
    "    - For numerical features, replace with with 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1f1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93e97f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(lead_source                 128\n",
       " industry                    134\n",
       " number_of_courses_viewed      0\n",
       " annual_income               181\n",
       " employment_status           100\n",
       " location                     63\n",
       " interaction_count             0\n",
       " lead_score                    0\n",
       " converted                     0\n",
       " dtype: int64,\n",
       " lead_source                 0\n",
       " industry                    0\n",
       " number_of_courses_viewed    0\n",
       " annual_income               0\n",
       " employment_status           0\n",
       " location                    0\n",
       " interaction_count           0\n",
       " lead_score                  0\n",
       " converted                   0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/course_lead_scoring.csv')\n",
    "\n",
    "# Missing values per column (before)\n",
    "missing_before = df.isna().sum()\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Fill missing values\n",
    "if len(categorical_cols) > 0:\n",
    "    df[categorical_cols] = df[categorical_cols].fillna('NA')\n",
    "if len(numeric_cols) > 0:\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0.0)\n",
    "\n",
    "missing_after = df.isna().sum()\n",
    "\n",
    "missing_before, missing_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dac78",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### What is the most frequent observation (mode) for the column industry?\n",
    "\n",
    "- NA\n",
    "- technology\n",
    "- healthcare\n",
    "- retail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5a315",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "405186bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retail'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_industry = df['industry'].mode()[0]\n",
    "mode_industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a533ee",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?\n",
    "\n",
    "- interaction_count and lead_score\n",
    "- number_of_courses_viewed and lead_score\n",
    "- number_of_courses_viewed and interaction_count\n",
    "- annual_income and interaction_count\n",
    "Only consider the pairs above when answering this question.\n",
    "\n",
    "Split the data\n",
    "- Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "- Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "- Make sure that the target value y is not in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ea3a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Columns found:\n",
      "['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score', 'converted']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "number_of_courses_viewed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "annual_income",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "interaction_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lead_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "converted",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b0dff7fb-263d-4797-be9f-31372131ed89",
       "rows": [
        [
         "number_of_courses_viewed",
         "1.0",
         "0.009770285756444567",
         "-0.023565222882888037",
         "-0.004878998354681276",
         "0.43591365802117926"
        ],
        [
         "annual_income",
         "0.009770285756444567",
         "1.0",
         "0.02703647240481443",
         "0.015609546050139008",
         "0.05313144169625205"
        ],
        [
         "interaction_count",
         "-0.023565222882888037",
         "0.02703647240481443",
         "1.0",
         "0.009888182496913131",
         "0.37457251779940415"
        ],
        [
         "lead_score",
         "-0.004878998354681276",
         "0.015609546050139008",
         "0.009888182496913131",
         "1.0",
         "0.19367349758690264"
        ],
        [
         "converted",
         "0.43591365802117926",
         "0.05313144169625205",
         "0.37457251779940415",
         "0.19367349758690264",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>-0.023565</td>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.435914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_income</th>\n",
       "      <td>0.009770</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.053131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interaction_count</th>\n",
       "      <td>-0.023565</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>0.374573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_score</th>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted</th>\n",
       "      <td>0.435914</td>\n",
       "      <td>0.053131</td>\n",
       "      <td>0.374573</td>\n",
       "      <td>0.193673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          number_of_courses_viewed  annual_income  \\\n",
       "number_of_courses_viewed                  1.000000       0.009770   \n",
       "annual_income                             0.009770       1.000000   \n",
       "interaction_count                        -0.023565       0.027036   \n",
       "lead_score                               -0.004879       0.015610   \n",
       "converted                                 0.435914       0.053131   \n",
       "\n",
       "                          interaction_count  lead_score  converted  \n",
       "number_of_courses_viewed          -0.023565   -0.004879   0.435914  \n",
       "annual_income                      0.027036    0.015610   0.053131  \n",
       "interaction_count                  1.000000    0.009888   0.374573  \n",
       "lead_score                         0.009888    1.000000   0.193673  \n",
       "converted                          0.374573    0.193673   1.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "print(\"Numeric Columns found:\")\n",
    "print(numeric_cols.tolist())\n",
    "\n",
    "df_numeric = df[numeric_cols]\n",
    "corr_matrix = df_numeric.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22c1abc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered correlations (excluding perfect correlations of 1.0):\n",
      "converted                 number_of_courses_viewed    0.435914\n",
      "interaction_count         converted                   0.374573\n",
      "lead_score                converted                   0.193673\n",
      "annual_income             converted                   0.053131\n",
      "                          interaction_count           0.027036\n",
      "                          lead_score                  0.015610\n",
      "lead_score                interaction_count           0.009888\n",
      "annual_income             number_of_courses_viewed    0.009770\n",
      "lead_score                number_of_courses_viewed   -0.004879\n",
      "number_of_courses_viewed  interaction_count          -0.023565\n",
      "dtype: float64\n",
      "\n",
      "Correlations of the specific pairs mentioned:\n",
      "interaction_count vs lead_score: 0.0099\n",
      "number_of_courses_viewed vs lead_score: -0.0049\n",
      "number_of_courses_viewed vs interaction_count: -0.0236\n",
      "annual_income vs interaction_count: 0.0270\n"
     ]
    }
   ],
   "source": [
    "correlations_sorted = corr_matrix.unstack().sort_values(ascending=False).drop_duplicates()\n",
    "print(\"Ordered correlations (excluding perfect correlations of 1.0):\")\n",
    "print(correlations_sorted[correlations_sorted < 1.0].head(10))\n",
    "\n",
    "pairs_to_check = [\n",
    "    ('interaction_count', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'lead_score'), \n",
    "    ('number_of_courses_viewed', 'interaction_count'),\n",
    "    ('annual_income', 'interaction_count')\n",
    "]\n",
    "\n",
    "print(\"\\nCorrelations of the specific pairs mentioned:\")\n",
    "for col1, col2 in pairs_to_check:\n",
    "    if col1 in corr_matrix.columns and col2 in corr_matrix.columns:\n",
    "        correlation = corr_matrix.loc[col1, col2]\n",
    "        print(f\"{col1} vs {col2}: {correlation:.4f}\")\n",
    "    else:\n",
    "        print(f\"Columnas {col1} o {col2} no encontradas en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "915c0b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_count and lead_score: 0.0099\n",
      "number_of_courses_viewed and lead_score: 0.0049\n",
      "number_of_courses_viewed and interaction_count: 0.0236\n",
      "annual_income and interaction_count: 0.0270\n",
      "\n",
      "'annual_income and interaction_count' con una correlación de 0.0270\n",
      "Original values (with sign):\n",
      "interaction_count and lead_score: 0.0099\n",
      "number_of_courses_viewed and lead_score: -0.0049\n",
      "number_of_courses_viewed and interaction_count: -0.0236\n",
      "annual_income and interaction_count: 0.0270\n"
     ]
    }
   ],
   "source": [
    "correlations = {\n",
    "    \"interaction_count and lead_score\": abs(corr_matrix.loc['interaction_count', 'lead_score']),\n",
    "    \"number_of_courses_viewed and lead_score\": abs(corr_matrix.loc['number_of_courses_viewed', 'lead_score']),\n",
    "    \"number_of_courses_viewed and interaction_count\": abs(corr_matrix.loc['number_of_courses_viewed', 'interaction_count']),\n",
    "    \"annual_income and interaction_count\": abs(corr_matrix.loc['annual_income', 'interaction_count'])\n",
    "}\n",
    "\n",
    "for pair, corr_value in correlations.items():\n",
    "    print(f\"{pair}: {corr_value:.4f}\")\n",
    "\n",
    "print()\n",
    "max_pair = max(correlations, key=correlations.get)\n",
    "max_correlation = correlations[max_pair]\n",
    "\n",
    "print(f\"'{max_pair}' con una correlación de {max_correlation:.4f}\")\n",
    "\n",
    "print(\"Original values (with sign):\")\n",
    "print(f\"interaction_count and lead_score: {corr_matrix.loc['interaction_count', 'lead_score']:.4f}\")\n",
    "print(f\"number_of_courses_viewed and lead_score: {corr_matrix.loc['number_of_courses_viewed', 'lead_score']:.4f}\")\n",
    "print(f\"number_of_courses_viewed and interaction_count: {corr_matrix.loc['number_of_courses_viewed', 'interaction_count']:.4f}\")\n",
    "print(f\"annual_income and interaction_count: {corr_matrix.loc['annual_income', 'interaction_count']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34e34e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b0ccc",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "Calculate the mutual information score between y and other categorical variables in the dataset. Use the training set only.\n",
    "Round the scores to 2 decimals using round(score, 2).\n",
    "Which of these variables has the biggest mutual information score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c551088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable: 'converted'\n",
      "Unique values ​​in 'converted': [0 1]\n",
      "Distribution: converted\n",
      "1    547\n",
      "0    329\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical columns to analyze: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "\n",
      "Processing column 'lead_source'...\n",
      "  Unique values: 6\n",
      "  Sample values: ['paid_ads' 'organic_search' 'NA' 'social_media' 'events']\n",
      "  Mutual Information Score: 0.04\n",
      "\n",
      "Processing column 'industry'...\n",
      "  Unique values: 8\n",
      "  Sample values: ['retail' 'manufacturing' 'technology' 'finance' 'NA']\n",
      "  Mutual Information Score: 0.01\n",
      "\n",
      "Processing column 'employment_status'...\n",
      "  Unique values: 5\n",
      "  Sample values: ['student' 'employed' 'unemployed' 'NA' 'self_employed']\n",
      "  Mutual Information Score: 0.01\n",
      "\n",
      "Processing column 'location'...\n",
      "  Unique values: 8\n",
      "  Sample values: ['middle_east' 'north_america' 'europe' 'australia' 'south_america']\n",
      "  Mutual Information Score: 0.0\n",
      "\n",
      "Mutual Information Scores (sorted from highest to lowest):\n",
      "lead_source: 0.04\n",
      "industry: 0.01\n",
      "employment_status: 0.01\n",
      "location: 0.0\n",
      "\n",
      "Variable with HIGHEST Mutual Information Score: 'lead_source' = 0.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "y_train = df_train['converted'].values\n",
    "print(\"Target variable: 'converted'\")\n",
    "print(\"Unique values ​​in 'converted':\", df_train['converted'].unique())\n",
    "print(\"Distribution:\", df_train['converted'].value_counts())\n",
    "print()\n",
    "\n",
    "categorical_cols = df_train.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col != 'converted']\n",
    "print(\"Categorical columns to analyze:\", categorical_cols)\n",
    "print()\n",
    "\n",
    "mi_scores = {}\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"Processing column '{col}'...\")\n",
    "    print(f\"  Unique values: {df_train[col].nunique()}\")\n",
    "    print(f\"  Sample values: {df_train[col].unique()[:5]}\")\n",
    "\n",
    "    X_encoded = le.fit_transform(df_train[col])\n",
    "    \n",
    "    mi = mutual_info_classif(X_encoded.reshape(-1, 1), y_train, discrete_features=True, random_state=42)\n",
    "    mi_scores[col] = round(mi[0], 2)\n",
    "    print(f\"  Mutual Information Score: {mi_scores[col]}\")\n",
    "    print()\n",
    "\n",
    "print(\"Mutual Information Scores (sorted from highest to lowest):\")\n",
    "for col, score in sorted(mi_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{col}: {score}\")\n",
    "\n",
    "max_var = max(mi_scores, key=mi_scores.get)\n",
    "max_score = mi_scores[max_var]\n",
    "print(f\"\\nVariable with HIGHEST Mutual Information Score: '{max_var}' = {max_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4c0e5",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Now let's train a logistic regression.\n",
    "Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "- Fit the model on the training dataset.\n",
    " - To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    " - model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "Calculate the accuracy on the validation dataset and round it to 2 decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c2dd688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "df_full_train = pd.get_dummies(df_full_train, drop_first=True)\n",
    "df_val = pd.get_dummies(df_val, drop_first=True)\n",
    "df_test = pd.get_dummies(df_test, drop_first=True)\n",
    "df_val = df_val.reindex(columns=df_full_train.columns, fill_value=0)\n",
    "df_test = df_test.reindex(columns=df_full_train.columns, fill_value=0)\n",
    "y_full_train = df_full_train['converted'].values\n",
    "y_val = df_val['converted'].values\n",
    "X_full_train = df_full_train.drop(columns=['converted']).values\n",
    "X_val = df_val.drop(columns=['converted']).values\n",
    "X_test = df_test.drop(columns=['converted']).values\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_full_train, y_full_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_accuracy = round(val_accuracy, 2)\n",
    "print(f\"Accuracy on the validation set: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698d64d",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "- Let's find the least useful feature using the feature elimination technique.\n",
    "- Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "- Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "Which of following feature has the smallest difference?\n",
    "\n",
    "- 'industry'\n",
    "- 'employment_status'\n",
    "- 'lead_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8155a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original precision with all the features: 0.7167\n",
      "Indices of features to evaluate: {'number_of_courses_viewed': 0, 'annual_income': 1, 'interaction_count': 2, 'lead_score': 3, 'lead_source_events': 4, 'lead_source_organic_search': 5, 'lead_source_paid_ads': 6, 'lead_source_referral': 7, 'lead_source_social_media': 8, 'industry_education': 9, 'industry_finance': 10, 'industry_healthcare': 11, 'industry_manufacturing': 12, 'industry_other': 13, 'industry_retail': 14, 'industry_technology': 15, 'employment_status_employed': 16, 'employment_status_self_employed': 17, 'employment_status_student': 18, 'employment_status_unemployed': 19, 'location_africa': 20, 'location_asia': 21, 'location_australia': 22, 'location_europe': 23, 'location_middle_east': 24, 'location_north_america': 25, 'location_south_america': 26}\n",
      "Feature 'industry' not found in the dataset.\n",
      "Feature 'employment_status' not found in the dataset.\n",
      "Evaluating removal of feature 'lead_score' (index 3)...\n",
      "  Precision without 'lead_score': 0.7099, Difference: 0.0068\n",
      "Least useful feature is: lead_score (Difference: 0.0068)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "def train_evaluate_model(X_train, y_train, X_val, y_val, model):\n",
    "    model_clone = clone(model)\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    y_val_pred = model_clone.predict(X_val)\n",
    "    return accuracy_score(y_val, y_val_pred)\n",
    "original_accuracy = train_evaluate_model(X_full_train, y_full_train, X_val, y_val, model)\n",
    "print(f\"Original precision with all the features: {original_accuracy:.4f}\")\n",
    "features_to_evaluate = ['industry', 'employment_status', 'lead_score']\n",
    "feature_indices = {col: idx for idx, col in enumerate(df_full_train.drop(columns=['converted']).columns)}\n",
    "print(\"Indices of features to evaluate:\", feature_indices)\n",
    "accuracy_differences = {}\n",
    "for feature in features_to_evaluate:\n",
    "    if feature in feature_indices:\n",
    "        idx = feature_indices[feature]\n",
    "        print(f\"Evaluating removal of feature '{feature}' (index {idx})...\")\n",
    "        X_train_reduced = np.delete(X_full_train, idx, axis=1)\n",
    "        X_val_reduced = np.delete(X_val, idx, axis=1)\n",
    "        reduced_accuracy = train_evaluate_model(X_train_reduced, y_full_train, X_val_reduced, y_val, model)\n",
    "        accuracy_difference = original_accuracy - reduced_accuracy\n",
    "        accuracy_differences[feature] = accuracy_difference\n",
    "        print(f\"  Precision without '{feature}': {reduced_accuracy:.4f}, Difference: {accuracy_difference:.4f}\")\n",
    "    else:\n",
    "        print(f\"Feature '{feature}' not found in the dataset.\")\n",
    "\n",
    "least_useful_feature = min(accuracy_differences, key=accuracy_differences.get)\n",
    "print(f\"Least useful feature is: {least_useful_feature} (Difference: {accuracy_differences[least_useful_feature]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b5b6e",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "- Now let's train a regularized logistic regression.\n",
    "- Let's try the following values of the parameter C: [0.01, 0.1, 1, 10, 100].\n",
    "- Train models using all the features as in Q4.\n",
    "- Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "- Which of these C leads to the best accuracy on the validation set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d50dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with C=0.01...\n",
      "Validation accuracy: 0.706\n",
      "Training model with C=0.1...\n",
      "Validation accuracy: 0.717\n",
      "Training model with C=1...\n",
      "Validation accuracy: 0.717\n",
      "Training model with C=10...\n",
      "Validation accuracy: 0.717\n",
      "Training model with C=100...\n",
      "Validation accuracy: 0.717\n",
      "Best C: 0.1, Accuracy: 0.717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "best_C = None\n",
    "best_accuracy = 0.0\n",
    "for C in C_values:\n",
    "    print(f\"Training model with C={C}...\")\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model.fit(X_full_train, y_full_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_accuracy_rounded = round(val_accuracy, 3)\n",
    "    print(f\"Validation accuracy: {val_accuracy_rounded}\")\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_C = C\n",
    "\n",
    "print(f\"Best C: {best_C}, Accuracy: {round(best_accuracy, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoocamp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
