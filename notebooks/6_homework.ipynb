{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dba7194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fb51d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/car_fuel_efficiency.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fef63fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5822, 14) (1941, 14) (1941, 14)\n",
      "14.993604027528097 14.95391385579406 14.991494510134533\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "# Preparation:\n",
    "\n",
    "# Fill missing values with zeros.\n",
    "# Do train/validation/test split with 60%/20%/20% distribution.\n",
    "# Use the train_test_split function and set the random_state parameter to 1.\n",
    "# Use DictVectorizer(sparse=True) to turn the dataframes into matrices.\n",
    "df = df.fillna(0)\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)  # 0.25 x 0.8 = 0.2\n",
    "y_train = df_train.fuel_efficiency_mpg.values\n",
    "y_val = df_val.fuel_efficiency_mpg.values\n",
    "y_test = df_test.fuel_efficiency_mpg.values\n",
    "del df_train['fuel_efficiency_mpg']\n",
    "del df_val['fuel_efficiency_mpg']\n",
    "del df_test['fuel_efficiency_mpg']\n",
    "\n",
    "dv = DictVectorizer(sparse=True)\n",
    "train_dicts = df_train.to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "val_dicts = df_val.to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)\n",
    "test_dicts = df_test.to_dict(orient='records')\n",
    "X_test = dv.transform(test_dicts)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.mean(), y_val.mean(), y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9e72d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important feature: vehicle_weight\n",
      "Feature importance: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "# Let's train a decision tree regressor to predict the fuel_efficiency_mpg variable.\n",
    "\n",
    "# Train a model with max_depth=1.\n",
    "# Which feature is used for splitting the data?\n",
    "\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=1)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Get feature names from DictVectorizer and find the most important feature\n",
    "feature_names = dv.get_feature_names_out()\n",
    "most_important_feature = feature_names[dt.feature_importances_.argmax()]\n",
    "print(f\"Most important feature: {most_important_feature}\")\n",
    "print(f\"Feature importance: {dt.feature_importances_.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "729983ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.46\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "# Train a random forest regressor with these parameters:\n",
    "\n",
    "# n_estimators=10\n",
    "# random_state=1\n",
    "# n_jobs=-1 (optional - to make training faster)\n",
    "# What's the RMSE of this model on the validation data?\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=10, random_state=1, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_val_pred = rf.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "print(f\"Validation RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe52a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 10, Validation RMSE: 0.460\n",
      "n_estimators: 20, Validation RMSE: 0.454\n",
      "n_estimators: 20, Validation RMSE: 0.454\n",
      "n_estimators: 30, Validation RMSE: 0.452\n",
      "n_estimators: 30, Validation RMSE: 0.452\n",
      "n_estimators: 40, Validation RMSE: 0.449\n",
      "n_estimators: 40, Validation RMSE: 0.449\n",
      "n_estimators: 50, Validation RMSE: 0.447\n",
      "n_estimators: 50, Validation RMSE: 0.447\n",
      "n_estimators: 60, Validation RMSE: 0.445\n",
      "n_estimators: 60, Validation RMSE: 0.445\n",
      "n_estimators: 70, Validation RMSE: 0.445\n",
      "n_estimators: 70, Validation RMSE: 0.445\n",
      "n_estimators: 80, Validation RMSE: 0.445\n",
      "n_estimators: 80, Validation RMSE: 0.445\n",
      "n_estimators: 90, Validation RMSE: 0.445\n",
      "n_estimators: 90, Validation RMSE: 0.445\n",
      "n_estimators: 100, Validation RMSE: 0.445\n",
      "n_estimators: 100, Validation RMSE: 0.445\n",
      "n_estimators: 110, Validation RMSE: 0.444\n",
      "n_estimators: 110, Validation RMSE: 0.444\n",
      "n_estimators: 120, Validation RMSE: 0.444\n",
      "Best n_estimators: 110 with RMSE: 0.444\n",
      "n_estimators: 120, Validation RMSE: 0.444\n",
      "Best n_estimators: 110 with RMSE: 0.444\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "# Now let's experiment with the n_estimators parameter\n",
    "\n",
    "# Try different values of this parameter from 10 to 200 with step 10.\n",
    "# Set random_state to 1.\n",
    "# Evaluate the model on the validation dataset.\n",
    "# After which value of n_estimators does RMSE stop improving?\n",
    "# Consider 3 decimal places for calculating the answer.\n",
    "\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_n_estimators = 0\n",
    "for n_estimators in range(10, 201, 10):\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, random_state=1, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    print(f\"n_estimators: {n_estimators}, Validation RMSE: {rmse:.3f}\")\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_n_estimators = n_estimators\n",
    "    else:\n",
    "        break\n",
    "print(f\"Best n_estimators: {best_n_estimators} with RMSE: {best_rmse:.3f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "# Let's select the best max_depth:\n",
    "\n",
    "# Try different values of max_depth: [10, 15, 20, 25]\n",
    "# For each of these values,\n",
    "# try different values of n_estimators from 10 till 200 (with step 10)\n",
    "# calculate the mean RMSE\n",
    "# Fix the random seed: random_state=1\n",
    "# What's the best max_depth, using the mean RMSE?\n",
    "\n",
    "\n",
    "best_overall_rmse = float('inf')\n",
    "best_overall_max_depth = 0\n",
    "for max_depth in [10, 15, 20, 25]:\n",
    "    for n_estimators in range(10, 201, 10):\n",
    "        rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=1, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_val_pred = rf.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        if rmse < best_overall_rmse:\n",
    "            best_overall_rmse = rmse\n",
    "            best_overall_max_depth = max_depth\n",
    "print(f\"Best max_depth: {best_overall_max_depth} with RMSE: {best_overall_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d28f1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important feature: vehicle_weight\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "# We can extract feature importance information from tree-based models.\n",
    "\n",
    "# At each step of the decision tree learning algorithm, it finds the best split.\n",
    "# When doing it, we can calculate \"gain\" - the reduction in impurity before and after the split.\n",
    "# This gain is quite useful in understanding what are the important features for tree-based models.\n",
    "\n",
    "# In Scikit-Learn, tree-based models contain this information in the\n",
    "# feature_importances_\n",
    "# field.\n",
    "\n",
    "# For this homework question, we'll find the most important feature:\n",
    "\n",
    "# Train the model with these parameters:\n",
    "# n_estimators=10,\n",
    "# max_depth=20,\n",
    "# random_state=1,\n",
    "# n_jobs=-1 (optional)\n",
    "# Get the feature importance information from this model\n",
    "# What's the most important feature (among these 4)?\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=1, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_names = dv.get_feature_names_out()\n",
    "importances = rf.feature_importances_\n",
    "most_important_feature = feature_names[importances.argmax()]\n",
    "print(f\"Most important feature: {most_important_feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d91d87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.81393\tval-rmse:1.85444\n",
      "[1]\ttrain-rmse:1.31919\tval-rmse:1.35353\n",
      "[2]\ttrain-rmse:0.98120\tval-rmse:1.01316\n",
      "[3]\ttrain-rmse:0.75443\tval-rmse:0.78667\n",
      "[4]\ttrain-rmse:0.60680\tval-rmse:0.64318\n",
      "[5]\ttrain-rmse:0.51381\tval-rmse:0.55664\n",
      "[6]\ttrain-rmse:0.45470\tval-rmse:0.50321\n",
      "[1]\ttrain-rmse:1.31919\tval-rmse:1.35353\n",
      "[2]\ttrain-rmse:0.98120\tval-rmse:1.01316\n",
      "[3]\ttrain-rmse:0.75443\tval-rmse:0.78667\n",
      "[4]\ttrain-rmse:0.60680\tval-rmse:0.64318\n",
      "[5]\ttrain-rmse:0.51381\tval-rmse:0.55664\n",
      "[6]\ttrain-rmse:0.45470\tval-rmse:0.50321\n",
      "[7]\ttrain-rmse:0.41881\tval-rmse:0.47254\n",
      "[7]\ttrain-rmse:0.41881\tval-rmse:0.47254\n",
      "[8]\ttrain-rmse:0.39534\tval-rmse:0.45509\n",
      "[9]\ttrain-rmse:0.38038\tval-rmse:0.44564\n",
      "[10]\ttrain-rmse:0.37115\tval-rmse:0.43896\n",
      "[8]\ttrain-rmse:0.39534\tval-rmse:0.45509\n",
      "[9]\ttrain-rmse:0.38038\tval-rmse:0.44564\n",
      "[10]\ttrain-rmse:0.37115\tval-rmse:0.43896\n",
      "[11]\ttrain-rmse:0.36361\tval-rmse:0.43594\n",
      "[12]\ttrain-rmse:0.35850\tval-rmse:0.43558\n",
      "[13]\ttrain-rmse:0.35365\tval-rmse:0.43394\n",
      "[11]\ttrain-rmse:0.36361\tval-rmse:0.43594\n",
      "[12]\ttrain-rmse:0.35850\tval-rmse:0.43558\n",
      "[13]\ttrain-rmse:0.35365\tval-rmse:0.43394\n",
      "[14]\ttrain-rmse:0.35025\tval-rmse:0.43349\n",
      "[15]\ttrain-rmse:0.34666\tval-rmse:0.43362\n",
      "[16]\ttrain-rmse:0.34459\tval-rmse:0.43378\n",
      "[17]\ttrain-rmse:0.34128\tval-rmse:0.43405\n",
      "[18]\ttrain-rmse:0.33822\tval-rmse:0.43391\n",
      "[19]\ttrain-rmse:0.33709\tval-rmse:0.43374\n",
      "[20]\ttrain-rmse:0.33553\tval-rmse:0.43376\n",
      "[21]\ttrain-rmse:0.33243\tval-rmse:0.43453\n",
      "[14]\ttrain-rmse:0.35025\tval-rmse:0.43349\n",
      "[15]\ttrain-rmse:0.34666\tval-rmse:0.43362\n",
      "[16]\ttrain-rmse:0.34459\tval-rmse:0.43378\n",
      "[17]\ttrain-rmse:0.34128\tval-rmse:0.43405\n",
      "[18]\ttrain-rmse:0.33822\tval-rmse:0.43391\n",
      "[19]\ttrain-rmse:0.33709\tval-rmse:0.43374\n",
      "[20]\ttrain-rmse:0.33553\tval-rmse:0.43376\n",
      "[21]\ttrain-rmse:0.33243\tval-rmse:0.43453\n",
      "[22]\ttrain-rmse:0.33031\tval-rmse:0.43510\n",
      "[23]\ttrain-rmse:0.32815\tval-rmse:0.43601\n",
      "[24]\ttrain-rmse:0.32670\tval-rmse:0.43592\n",
      "[25]\ttrain-rmse:0.32268\tval-rmse:0.43683\n",
      "[26]\ttrain-rmse:0.32085\tval-rmse:0.43678\n",
      "[27]\ttrain-rmse:0.32035\tval-rmse:0.43681\n",
      "[22]\ttrain-rmse:0.33031\tval-rmse:0.43510\n",
      "[23]\ttrain-rmse:0.32815\tval-rmse:0.43601\n",
      "[24]\ttrain-rmse:0.32670\tval-rmse:0.43592\n",
      "[25]\ttrain-rmse:0.32268\tval-rmse:0.43683\n",
      "[26]\ttrain-rmse:0.32085\tval-rmse:0.43678\n",
      "[27]\ttrain-rmse:0.32035\tval-rmse:0.43681\n",
      "[28]\ttrain-rmse:0.31879\tval-rmse:0.43719\n",
      "[29]\ttrain-rmse:0.31653\tval-rmse:0.43739\n",
      "[30]\ttrain-rmse:0.31475\tval-rmse:0.43752\n",
      "[31]\ttrain-rmse:0.31392\tval-rmse:0.43782\n",
      "[32]\ttrain-rmse:0.31263\tval-rmse:0.43761\n",
      "[33]\ttrain-rmse:0.31155\tval-rmse:0.43764\n",
      "[34]\ttrain-rmse:0.31054\tval-rmse:0.43793\n",
      "[35]\ttrain-rmse:0.30960\tval-rmse:0.43784\n",
      "[28]\ttrain-rmse:0.31879\tval-rmse:0.43719\n",
      "[29]\ttrain-rmse:0.31653\tval-rmse:0.43739\n",
      "[30]\ttrain-rmse:0.31475\tval-rmse:0.43752\n",
      "[31]\ttrain-rmse:0.31392\tval-rmse:0.43782\n",
      "[32]\ttrain-rmse:0.31263\tval-rmse:0.43761\n",
      "[33]\ttrain-rmse:0.31155\tval-rmse:0.43764\n",
      "[34]\ttrain-rmse:0.31054\tval-rmse:0.43793\n",
      "[35]\ttrain-rmse:0.30960\tval-rmse:0.43784\n",
      "[36]\ttrain-rmse:0.30813\tval-rmse:0.43828\n",
      "[37]\ttrain-rmse:0.30573\tval-rmse:0.43891\n",
      "[38]\ttrain-rmse:0.30406\tval-rmse:0.43924\n",
      "[39]\ttrain-rmse:0.30257\tval-rmse:0.43946\n",
      "[40]\ttrain-rmse:0.30202\tval-rmse:0.43968\n",
      "[41]\ttrain-rmse:0.29908\tval-rmse:0.43978\n",
      "[42]\ttrain-rmse:0.29772\tval-rmse:0.43998\n",
      "[43]\ttrain-rmse:0.29594\tval-rmse:0.43969\n",
      "[36]\ttrain-rmse:0.30813\tval-rmse:0.43828\n",
      "[37]\ttrain-rmse:0.30573\tval-rmse:0.43891\n",
      "[38]\ttrain-rmse:0.30406\tval-rmse:0.43924\n",
      "[39]\ttrain-rmse:0.30257\tval-rmse:0.43946\n",
      "[40]\ttrain-rmse:0.30202\tval-rmse:0.43968\n",
      "[41]\ttrain-rmse:0.29908\tval-rmse:0.43978\n",
      "[42]\ttrain-rmse:0.29772\tval-rmse:0.43998\n",
      "[43]\ttrain-rmse:0.29594\tval-rmse:0.43969\n",
      "[44]\ttrain-rmse:0.29420\tval-rmse:0.44009\n",
      "[45]\ttrain-rmse:0.29126\tval-rmse:0.44024\n",
      "[46]\ttrain-rmse:0.28929\tval-rmse:0.44041\n",
      "[47]\ttrain-rmse:0.28821\tval-rmse:0.44101\n",
      "[48]\ttrain-rmse:0.28629\tval-rmse:0.44096\n",
      "[49]\ttrain-rmse:0.28528\tval-rmse:0.44142\n",
      "[50]\ttrain-rmse:0.28456\tval-rmse:0.44140\n",
      "[51]\ttrain-rmse:0.28309\tval-rmse:0.44160\n",
      "[44]\ttrain-rmse:0.29420\tval-rmse:0.44009\n",
      "[45]\ttrain-rmse:0.29126\tval-rmse:0.44024\n",
      "[46]\ttrain-rmse:0.28929\tval-rmse:0.44041\n",
      "[47]\ttrain-rmse:0.28821\tval-rmse:0.44101\n",
      "[48]\ttrain-rmse:0.28629\tval-rmse:0.44096\n",
      "[49]\ttrain-rmse:0.28528\tval-rmse:0.44142\n",
      "[50]\ttrain-rmse:0.28456\tval-rmse:0.44140\n",
      "[51]\ttrain-rmse:0.28309\tval-rmse:0.44160\n",
      "[52]\ttrain-rmse:0.28144\tval-rmse:0.44109\n",
      "[53]\ttrain-rmse:0.28095\tval-rmse:0.44152\n",
      "[54]\ttrain-rmse:0.27894\tval-rmse:0.44191\n",
      "[55]\ttrain-rmse:0.27618\tval-rmse:0.44225\n",
      "[56]\ttrain-rmse:0.27373\tval-rmse:0.44265\n",
      "[57]\ttrain-rmse:0.27156\tval-rmse:0.44310\n",
      "[58]\ttrain-rmse:0.26938\tval-rmse:0.44276\n",
      "[59]\ttrain-rmse:0.26797\tval-rmse:0.44282\n",
      "[52]\ttrain-rmse:0.28144\tval-rmse:0.44109\n",
      "[53]\ttrain-rmse:0.28095\tval-rmse:0.44152\n",
      "[54]\ttrain-rmse:0.27894\tval-rmse:0.44191\n",
      "[55]\ttrain-rmse:0.27618\tval-rmse:0.44225\n",
      "[56]\ttrain-rmse:0.27373\tval-rmse:0.44265\n",
      "[57]\ttrain-rmse:0.27156\tval-rmse:0.44310\n",
      "[58]\ttrain-rmse:0.26938\tval-rmse:0.44276\n",
      "[59]\ttrain-rmse:0.26797\tval-rmse:0.44282\n",
      "[60]\ttrain-rmse:0.26768\tval-rmse:0.44290\n",
      "[61]\ttrain-rmse:0.26749\tval-rmse:0.44285\n",
      "[62]\ttrain-rmse:0.26536\tval-rmse:0.44311\n",
      "[63]\ttrain-rmse:0.26420\tval-rmse:0.44329\n",
      "[64]\ttrain-rmse:0.26236\tval-rmse:0.44360\n",
      "[65]\ttrain-rmse:0.26174\tval-rmse:0.44352\n",
      "[66]\ttrain-rmse:0.26155\tval-rmse:0.44360\n",
      "[67]\ttrain-rmse:0.25959\tval-rmse:0.44410\n",
      "[60]\ttrain-rmse:0.26768\tval-rmse:0.44290\n",
      "[61]\ttrain-rmse:0.26749\tval-rmse:0.44285\n",
      "[62]\ttrain-rmse:0.26536\tval-rmse:0.44311\n",
      "[63]\ttrain-rmse:0.26420\tval-rmse:0.44329\n",
      "[64]\ttrain-rmse:0.26236\tval-rmse:0.44360\n",
      "[65]\ttrain-rmse:0.26174\tval-rmse:0.44352\n",
      "[66]\ttrain-rmse:0.26155\tval-rmse:0.44360\n",
      "[67]\ttrain-rmse:0.25959\tval-rmse:0.44410\n",
      "[68]\ttrain-rmse:0.25719\tval-rmse:0.44487\n",
      "[69]\ttrain-rmse:0.25665\tval-rmse:0.44476\n",
      "[70]\ttrain-rmse:0.25489\tval-rmse:0.44531\n",
      "[71]\ttrain-rmse:0.25277\tval-rmse:0.44563\n",
      "[72]\ttrain-rmse:0.25084\tval-rmse:0.44632\n",
      "[73]\ttrain-rmse:0.25017\tval-rmse:0.44649\n",
      "[68]\ttrain-rmse:0.25719\tval-rmse:0.44487\n",
      "[69]\ttrain-rmse:0.25665\tval-rmse:0.44476\n",
      "[70]\ttrain-rmse:0.25489\tval-rmse:0.44531\n",
      "[71]\ttrain-rmse:0.25277\tval-rmse:0.44563\n",
      "[72]\ttrain-rmse:0.25084\tval-rmse:0.44632\n",
      "[73]\ttrain-rmse:0.25017\tval-rmse:0.44649\n",
      "[74]\ttrain-rmse:0.24985\tval-rmse:0.44661\n",
      "[75]\ttrain-rmse:0.24792\tval-rmse:0.44628\n",
      "[76]\ttrain-rmse:0.24643\tval-rmse:0.44671\n",
      "[77]\ttrain-rmse:0.24565\tval-rmse:0.44667\n",
      "[78]\ttrain-rmse:0.24421\tval-rmse:0.44633\n",
      "[79]\ttrain-rmse:0.24291\tval-rmse:0.44663\n",
      "[80]\ttrain-rmse:0.24254\tval-rmse:0.44689\n",
      "[74]\ttrain-rmse:0.24985\tval-rmse:0.44661\n",
      "[75]\ttrain-rmse:0.24792\tval-rmse:0.44628\n",
      "[76]\ttrain-rmse:0.24643\tval-rmse:0.44671\n",
      "[77]\ttrain-rmse:0.24565\tval-rmse:0.44667\n",
      "[78]\ttrain-rmse:0.24421\tval-rmse:0.44633\n",
      "[79]\ttrain-rmse:0.24291\tval-rmse:0.44663\n",
      "[80]\ttrain-rmse:0.24254\tval-rmse:0.44689\n",
      "[81]\ttrain-rmse:0.24246\tval-rmse:0.44692\n",
      "[82]\ttrain-rmse:0.24112\tval-rmse:0.44705\n",
      "[83]\ttrain-rmse:0.23963\tval-rmse:0.44721\n",
      "[84]\ttrain-rmse:0.23803\tval-rmse:0.44704\n",
      "[85]\ttrain-rmse:0.23644\tval-rmse:0.44749\n",
      "[86]\ttrain-rmse:0.23533\tval-rmse:0.44757\n",
      "[87]\ttrain-rmse:0.23481\tval-rmse:0.44756\n",
      "[81]\ttrain-rmse:0.24246\tval-rmse:0.44692\n",
      "[82]\ttrain-rmse:0.24112\tval-rmse:0.44705\n",
      "[83]\ttrain-rmse:0.23963\tval-rmse:0.44721\n",
      "[84]\ttrain-rmse:0.23803\tval-rmse:0.44704\n",
      "[85]\ttrain-rmse:0.23644\tval-rmse:0.44749\n",
      "[86]\ttrain-rmse:0.23533\tval-rmse:0.44757\n",
      "[87]\ttrain-rmse:0.23481\tval-rmse:0.44756\n",
      "[88]\ttrain-rmse:0.23428\tval-rmse:0.44778\n",
      "[89]\ttrain-rmse:0.23267\tval-rmse:0.44809\n",
      "[90]\ttrain-rmse:0.23193\tval-rmse:0.44839\n",
      "[91]\ttrain-rmse:0.23149\tval-rmse:0.44844\n",
      "[92]\ttrain-rmse:0.23040\tval-rmse:0.44863\n",
      "[93]\ttrain-rmse:0.22891\tval-rmse:0.44896\n",
      "[94]\ttrain-rmse:0.22701\tval-rmse:0.44922\n",
      "[95]\ttrain-rmse:0.22475\tval-rmse:0.44904\n",
      "[88]\ttrain-rmse:0.23428\tval-rmse:0.44778\n",
      "[89]\ttrain-rmse:0.23267\tval-rmse:0.44809\n",
      "[90]\ttrain-rmse:0.23193\tval-rmse:0.44839\n",
      "[91]\ttrain-rmse:0.23149\tval-rmse:0.44844\n",
      "[92]\ttrain-rmse:0.23040\tval-rmse:0.44863\n",
      "[93]\ttrain-rmse:0.22891\tval-rmse:0.44896\n",
      "[94]\ttrain-rmse:0.22701\tval-rmse:0.44922\n",
      "[95]\ttrain-rmse:0.22475\tval-rmse:0.44904\n",
      "[96]\ttrain-rmse:0.22336\tval-rmse:0.44954\n",
      "[97]\ttrain-rmse:0.22131\tval-rmse:0.44979\n",
      "[98]\ttrain-rmse:0.22013\tval-rmse:0.45040\n",
      "[99]\ttrain-rmse:0.21950\tval-rmse:0.45018\n",
      "eta: 0.3, Validation RMSE: 0.450\n",
      "[96]\ttrain-rmse:0.22336\tval-rmse:0.44954\n",
      "[97]\ttrain-rmse:0.22131\tval-rmse:0.44979\n",
      "[98]\ttrain-rmse:0.22013\tval-rmse:0.45040\n",
      "[99]\ttrain-rmse:0.21950\tval-rmse:0.45018\n",
      "eta: 0.3, Validation RMSE: 0.450\n",
      "[0]\ttrain-rmse:2.28944\tval-rmse:2.34561\n",
      "[1]\ttrain-rmse:2.07396\tval-rmse:2.12434\n",
      "[2]\ttrain-rmse:1.88066\tval-rmse:1.92597\n",
      "[3]\ttrain-rmse:1.70730\tval-rmse:1.74987\n",
      "[4]\ttrain-rmse:1.55163\tval-rmse:1.59059\n",
      "[5]\ttrain-rmse:1.41247\tval-rmse:1.44988\n",
      "[6]\ttrain-rmse:1.28796\tval-rmse:1.32329\n",
      "[0]\ttrain-rmse:2.28944\tval-rmse:2.34561\n",
      "[1]\ttrain-rmse:2.07396\tval-rmse:2.12434\n",
      "[2]\ttrain-rmse:1.88066\tval-rmse:1.92597\n",
      "[3]\ttrain-rmse:1.70730\tval-rmse:1.74987\n",
      "[4]\ttrain-rmse:1.55163\tval-rmse:1.59059\n",
      "[5]\ttrain-rmse:1.41247\tval-rmse:1.44988\n",
      "[6]\ttrain-rmse:1.28796\tval-rmse:1.32329\n",
      "[7]\ttrain-rmse:1.17660\tval-rmse:1.20930\n",
      "[8]\ttrain-rmse:1.07736\tval-rmse:1.10830\n",
      "[9]\ttrain-rmse:0.98883\tval-rmse:1.02009\n",
      "[10]\ttrain-rmse:0.91008\tval-rmse:0.94062\n",
      "[11]\ttrain-rmse:0.84030\tval-rmse:0.87100\n",
      "[12]\ttrain-rmse:0.77874\tval-rmse:0.80916\n",
      "[13]\ttrain-rmse:0.72417\tval-rmse:0.75465\n",
      "[14]\ttrain-rmse:0.67626\tval-rmse:0.70780\n",
      "[7]\ttrain-rmse:1.17660\tval-rmse:1.20930\n",
      "[8]\ttrain-rmse:1.07736\tval-rmse:1.10830\n",
      "[9]\ttrain-rmse:0.98883\tval-rmse:1.02009\n",
      "[10]\ttrain-rmse:0.91008\tval-rmse:0.94062\n",
      "[11]\ttrain-rmse:0.84030\tval-rmse:0.87100\n",
      "[12]\ttrain-rmse:0.77874\tval-rmse:0.80916\n",
      "[13]\ttrain-rmse:0.72417\tval-rmse:0.75465\n",
      "[14]\ttrain-rmse:0.67626\tval-rmse:0.70780\n",
      "[15]\ttrain-rmse:0.63402\tval-rmse:0.66672\n",
      "[16]\ttrain-rmse:0.59690\tval-rmse:0.63062\n",
      "[17]\ttrain-rmse:0.56447\tval-rmse:0.60016\n",
      "[18]\ttrain-rmse:0.53619\tval-rmse:0.57383\n",
      "[19]\ttrain-rmse:0.51138\tval-rmse:0.55044\n",
      "[20]\ttrain-rmse:0.48983\tval-rmse:0.53064\n",
      "[21]\ttrain-rmse:0.47135\tval-rmse:0.51451\n",
      "[15]\ttrain-rmse:0.63402\tval-rmse:0.66672\n",
      "[16]\ttrain-rmse:0.59690\tval-rmse:0.63062\n",
      "[17]\ttrain-rmse:0.56447\tval-rmse:0.60016\n",
      "[18]\ttrain-rmse:0.53619\tval-rmse:0.57383\n",
      "[19]\ttrain-rmse:0.51138\tval-rmse:0.55044\n",
      "[20]\ttrain-rmse:0.48983\tval-rmse:0.53064\n",
      "[21]\ttrain-rmse:0.47135\tval-rmse:0.51451\n",
      "[22]\ttrain-rmse:0.45501\tval-rmse:0.49998\n",
      "[23]\ttrain-rmse:0.44120\tval-rmse:0.48790\n",
      "[24]\ttrain-rmse:0.42929\tval-rmse:0.47773\n",
      "[25]\ttrain-rmse:0.41881\tval-rmse:0.46891\n",
      "[26]\ttrain-rmse:0.40953\tval-rmse:0.46151\n",
      "[27]\ttrain-rmse:0.40173\tval-rmse:0.45551\n",
      "[22]\ttrain-rmse:0.45501\tval-rmse:0.49998\n",
      "[23]\ttrain-rmse:0.44120\tval-rmse:0.48790\n",
      "[24]\ttrain-rmse:0.42929\tval-rmse:0.47773\n",
      "[25]\ttrain-rmse:0.41881\tval-rmse:0.46891\n",
      "[26]\ttrain-rmse:0.40953\tval-rmse:0.46151\n",
      "[27]\ttrain-rmse:0.40173\tval-rmse:0.45551\n",
      "[28]\ttrain-rmse:0.39470\tval-rmse:0.45043\n",
      "[29]\ttrain-rmse:0.38873\tval-rmse:0.44621\n",
      "[30]\ttrain-rmse:0.38342\tval-rmse:0.44289\n",
      "[31]\ttrain-rmse:0.37876\tval-rmse:0.43989\n",
      "[32]\ttrain-rmse:0.37450\tval-rmse:0.43754\n",
      "[33]\ttrain-rmse:0.37073\tval-rmse:0.43553\n",
      "[34]\ttrain-rmse:0.36743\tval-rmse:0.43390\n",
      "[35]\ttrain-rmse:0.36435\tval-rmse:0.43250\n",
      "[28]\ttrain-rmse:0.39470\tval-rmse:0.45043\n",
      "[29]\ttrain-rmse:0.38873\tval-rmse:0.44621\n",
      "[30]\ttrain-rmse:0.38342\tval-rmse:0.44289\n",
      "[31]\ttrain-rmse:0.37876\tval-rmse:0.43989\n",
      "[32]\ttrain-rmse:0.37450\tval-rmse:0.43754\n",
      "[33]\ttrain-rmse:0.37073\tval-rmse:0.43553\n",
      "[34]\ttrain-rmse:0.36743\tval-rmse:0.43390\n",
      "[35]\ttrain-rmse:0.36435\tval-rmse:0.43250\n",
      "[36]\ttrain-rmse:0.36178\tval-rmse:0.43095\n",
      "[37]\ttrain-rmse:0.35927\tval-rmse:0.42976\n",
      "[38]\ttrain-rmse:0.35713\tval-rmse:0.42866\n",
      "[39]\ttrain-rmse:0.35506\tval-rmse:0.42806\n",
      "[40]\ttrain-rmse:0.35343\tval-rmse:0.42746\n",
      "[41]\ttrain-rmse:0.35195\tval-rmse:0.42692\n",
      "[42]\ttrain-rmse:0.35024\tval-rmse:0.42666\n",
      "[43]\ttrain-rmse:0.34862\tval-rmse:0.42616\n",
      "[36]\ttrain-rmse:0.36178\tval-rmse:0.43095\n",
      "[37]\ttrain-rmse:0.35927\tval-rmse:0.42976\n",
      "[38]\ttrain-rmse:0.35713\tval-rmse:0.42866\n",
      "[39]\ttrain-rmse:0.35506\tval-rmse:0.42806\n",
      "[40]\ttrain-rmse:0.35343\tval-rmse:0.42746\n",
      "[41]\ttrain-rmse:0.35195\tval-rmse:0.42692\n",
      "[42]\ttrain-rmse:0.35024\tval-rmse:0.42666\n",
      "[43]\ttrain-rmse:0.34862\tval-rmse:0.42616\n",
      "[44]\ttrain-rmse:0.34714\tval-rmse:0.42613\n",
      "[45]\ttrain-rmse:0.34621\tval-rmse:0.42595\n",
      "[46]\ttrain-rmse:0.34477\tval-rmse:0.42563\n",
      "[44]\ttrain-rmse:0.34714\tval-rmse:0.42613\n",
      "[45]\ttrain-rmse:0.34621\tval-rmse:0.42595\n",
      "[46]\ttrain-rmse:0.34477\tval-rmse:0.42563\n",
      "[47]\ttrain-rmse:0.34342\tval-rmse:0.42548\n",
      "[48]\ttrain-rmse:0.34217\tval-rmse:0.42520\n",
      "[47]\ttrain-rmse:0.34342\tval-rmse:0.42548\n",
      "[48]\ttrain-rmse:0.34217\tval-rmse:0.42520\n",
      "[49]\ttrain-rmse:0.34097\tval-rmse:0.42513\n",
      "[50]\ttrain-rmse:0.33998\tval-rmse:0.42498\n",
      "[51]\ttrain-rmse:0.33860\tval-rmse:0.42481\n",
      "[52]\ttrain-rmse:0.33767\tval-rmse:0.42453\n",
      "[53]\ttrain-rmse:0.33651\tval-rmse:0.42459\n",
      "[54]\ttrain-rmse:0.33560\tval-rmse:0.42448\n",
      "[55]\ttrain-rmse:0.33480\tval-rmse:0.42449\n",
      "[49]\ttrain-rmse:0.34097\tval-rmse:0.42513\n",
      "[50]\ttrain-rmse:0.33998\tval-rmse:0.42498\n",
      "[51]\ttrain-rmse:0.33860\tval-rmse:0.42481\n",
      "[52]\ttrain-rmse:0.33767\tval-rmse:0.42453\n",
      "[53]\ttrain-rmse:0.33651\tval-rmse:0.42459\n",
      "[54]\ttrain-rmse:0.33560\tval-rmse:0.42448\n",
      "[55]\ttrain-rmse:0.33480\tval-rmse:0.42449\n",
      "[56]\ttrain-rmse:0.33386\tval-rmse:0.42426\n",
      "[57]\ttrain-rmse:0.33292\tval-rmse:0.42429\n",
      "[58]\ttrain-rmse:0.33196\tval-rmse:0.42438\n",
      "[59]\ttrain-rmse:0.33105\tval-rmse:0.42450\n",
      "[56]\ttrain-rmse:0.33386\tval-rmse:0.42426\n",
      "[57]\ttrain-rmse:0.33292\tval-rmse:0.42429\n",
      "[58]\ttrain-rmse:0.33196\tval-rmse:0.42438\n",
      "[59]\ttrain-rmse:0.33105\tval-rmse:0.42450\n",
      "[60]\ttrain-rmse:0.33054\tval-rmse:0.42456\n",
      "[61]\ttrain-rmse:0.32953\tval-rmse:0.42444\n",
      "[62]\ttrain-rmse:0.32872\tval-rmse:0.42454\n",
      "[63]\ttrain-rmse:0.32777\tval-rmse:0.42461\n",
      "[64]\ttrain-rmse:0.32673\tval-rmse:0.42479\n",
      "[65]\ttrain-rmse:0.32602\tval-rmse:0.42493\n",
      "[66]\ttrain-rmse:0.32489\tval-rmse:0.42520\n",
      "[67]\ttrain-rmse:0.32397\tval-rmse:0.42525\n",
      "[60]\ttrain-rmse:0.33054\tval-rmse:0.42456\n",
      "[61]\ttrain-rmse:0.32953\tval-rmse:0.42444\n",
      "[62]\ttrain-rmse:0.32872\tval-rmse:0.42454\n",
      "[63]\ttrain-rmse:0.32777\tval-rmse:0.42461\n",
      "[64]\ttrain-rmse:0.32673\tval-rmse:0.42479\n",
      "[65]\ttrain-rmse:0.32602\tval-rmse:0.42493\n",
      "[66]\ttrain-rmse:0.32489\tval-rmse:0.42520\n",
      "[67]\ttrain-rmse:0.32397\tval-rmse:0.42525\n",
      "[68]\ttrain-rmse:0.32338\tval-rmse:0.42518\n",
      "[69]\ttrain-rmse:0.32275\tval-rmse:0.42513\n",
      "[70]\ttrain-rmse:0.32202\tval-rmse:0.42503\n",
      "[71]\ttrain-rmse:0.32161\tval-rmse:0.42515\n",
      "[72]\ttrain-rmse:0.32060\tval-rmse:0.42496\n",
      "[73]\ttrain-rmse:0.31996\tval-rmse:0.42517\n",
      "[74]\ttrain-rmse:0.31963\tval-rmse:0.42513\n",
      "[68]\ttrain-rmse:0.32338\tval-rmse:0.42518\n",
      "[69]\ttrain-rmse:0.32275\tval-rmse:0.42513\n",
      "[70]\ttrain-rmse:0.32202\tval-rmse:0.42503\n",
      "[71]\ttrain-rmse:0.32161\tval-rmse:0.42515\n",
      "[72]\ttrain-rmse:0.32060\tval-rmse:0.42496\n",
      "[73]\ttrain-rmse:0.31996\tval-rmse:0.42517\n",
      "[74]\ttrain-rmse:0.31963\tval-rmse:0.42513\n",
      "[75]\ttrain-rmse:0.31895\tval-rmse:0.42526\n",
      "[76]\ttrain-rmse:0.31825\tval-rmse:0.42538\n",
      "[77]\ttrain-rmse:0.31755\tval-rmse:0.42559\n",
      "[78]\ttrain-rmse:0.31742\tval-rmse:0.42568\n",
      "[79]\ttrain-rmse:0.31702\tval-rmse:0.42568\n",
      "[80]\ttrain-rmse:0.31667\tval-rmse:0.42563\n",
      "[75]\ttrain-rmse:0.31895\tval-rmse:0.42526\n",
      "[76]\ttrain-rmse:0.31825\tval-rmse:0.42538\n",
      "[77]\ttrain-rmse:0.31755\tval-rmse:0.42559\n",
      "[78]\ttrain-rmse:0.31742\tval-rmse:0.42568\n",
      "[79]\ttrain-rmse:0.31702\tval-rmse:0.42568\n",
      "[80]\ttrain-rmse:0.31667\tval-rmse:0.42563\n",
      "[81]\ttrain-rmse:0.31656\tval-rmse:0.42571\n",
      "[82]\ttrain-rmse:0.31569\tval-rmse:0.42564\n",
      "[83]\ttrain-rmse:0.31547\tval-rmse:0.42568\n",
      "[84]\ttrain-rmse:0.31475\tval-rmse:0.42581\n",
      "[85]\ttrain-rmse:0.31440\tval-rmse:0.42574\n",
      "[86]\ttrain-rmse:0.31352\tval-rmse:0.42579\n",
      "[87]\ttrain-rmse:0.31285\tval-rmse:0.42576\n",
      "[81]\ttrain-rmse:0.31656\tval-rmse:0.42571\n",
      "[82]\ttrain-rmse:0.31569\tval-rmse:0.42564\n",
      "[83]\ttrain-rmse:0.31547\tval-rmse:0.42568\n",
      "[84]\ttrain-rmse:0.31475\tval-rmse:0.42581\n",
      "[85]\ttrain-rmse:0.31440\tval-rmse:0.42574\n",
      "[86]\ttrain-rmse:0.31352\tval-rmse:0.42579\n",
      "[87]\ttrain-rmse:0.31285\tval-rmse:0.42576\n",
      "[88]\ttrain-rmse:0.31243\tval-rmse:0.42586\n",
      "[89]\ttrain-rmse:0.31146\tval-rmse:0.42574\n",
      "[90]\ttrain-rmse:0.31059\tval-rmse:0.42586\n",
      "[91]\ttrain-rmse:0.30997\tval-rmse:0.42591\n",
      "[92]\ttrain-rmse:0.30943\tval-rmse:0.42590\n",
      "[93]\ttrain-rmse:0.30817\tval-rmse:0.42591\n",
      "[94]\ttrain-rmse:0.30699\tval-rmse:0.42622\n",
      "[88]\ttrain-rmse:0.31243\tval-rmse:0.42586\n",
      "[89]\ttrain-rmse:0.31146\tval-rmse:0.42574\n",
      "[90]\ttrain-rmse:0.31059\tval-rmse:0.42586\n",
      "[91]\ttrain-rmse:0.30997\tval-rmse:0.42591\n",
      "[92]\ttrain-rmse:0.30943\tval-rmse:0.42590\n",
      "[93]\ttrain-rmse:0.30817\tval-rmse:0.42591\n",
      "[94]\ttrain-rmse:0.30699\tval-rmse:0.42622\n",
      "[95]\ttrain-rmse:0.30625\tval-rmse:0.42611\n",
      "[96]\ttrain-rmse:0.30565\tval-rmse:0.42621\n",
      "[97]\ttrain-rmse:0.30557\tval-rmse:0.42629\n",
      "[98]\ttrain-rmse:0.30486\tval-rmse:0.42629\n",
      "[99]\ttrain-rmse:0.30419\tval-rmse:0.42623\n",
      "[95]\ttrain-rmse:0.30625\tval-rmse:0.42611\n",
      "[96]\ttrain-rmse:0.30565\tval-rmse:0.42621\n",
      "[97]\ttrain-rmse:0.30557\tval-rmse:0.42629\n",
      "[98]\ttrain-rmse:0.30486\tval-rmse:0.42629\n",
      "[99]\ttrain-rmse:0.30419\tval-rmse:0.42623\n",
      "eta: 0.1, Validation RMSE: 0.426\n",
      "eta: 0.1, Validation RMSE: 0.426\n"
     ]
    }
   ],
   "source": [
    "# Question 6\n",
    "# Now let's train an XGBoost model! For this question, we'll tune the eta parameter:\n",
    "\n",
    "# Install XGBoost\n",
    "# Create DMatrix for train and validation\n",
    "# Create a watchlist\n",
    "# Train a model with these parameters for 100 rounds:\n",
    "# xgb_params = {\n",
    "#     'eta': 0.3, \n",
    "#     'max_depth': 6,\n",
    "#     'min_child_weight': 1,\n",
    "    \n",
    "#     'objective': 'reg:squarederror',\n",
    "#     'nthread': 8,\n",
    "    \n",
    "#     'seed': 1,\n",
    "#     'verbosity': 1,\n",
    "# }\n",
    "# Now change eta from 0.3 to 0.1.\n",
    "\n",
    "# Which eta leads to the best RMSE score on the validation dataset?\n",
    "\n",
    "# 0.3\n",
    "# 0.1\n",
    "# Both give equal value\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "watchlist = [(dtrain, 'train'), (dval, 'val')]\n",
    "for eta in [0.3, 0.1]:\n",
    "    xgb_params = {\n",
    "        'eta': eta,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 1,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'nthread': 8,\n",
    "        'seed': 1,\n",
    "        'verbosity': 1,\n",
    "    }\n",
    "    model = xgb.train(xgb_params, dtrain, num_boost_round=100, evals=watchlist)\n",
    "    y_val_pred = model.predict(dval)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    print(f\"eta: {eta}, Validation RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0a1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoocamp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
